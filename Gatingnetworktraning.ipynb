{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5551c8-6ab4-4058-8e34-104452d8f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Expert Models...\n",
      "‚úì Expert 1 (URL) loaded\n",
      "‚úì Expert 2 (Text) loaded\n",
      "\n",
      "‚úÖ Both expert models loaded!\n",
      "\n",
      "Loading datasets...\n",
      "UTF-8 failed, trying latin-1 encoding...\n",
      "URL dataset: 722802 samples\n",
      "Text dataset: 150355 samples\n",
      "Combined dataset: 50000 samples\n",
      "Label distribution:\n",
      "label\n",
      "0.0    32443\n",
      "1.0    17557\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 40000 samples\n",
      "Test set: 10000 samples\n",
      "\n",
      "Training Gating Network...\n",
      "======================================================================\n",
      "  Epoch 1/5 - Sample 1000/40000 - Loss: 0.3143\n",
      "  Epoch 1/5 - Sample 2000/40000 - Loss: 0.5039\n",
      "  Epoch 1/5 - Sample 3000/40000 - Loss: 0.3153\n",
      "  Epoch 1/5 - Sample 4000/40000 - Loss: 0.3133\n",
      "  Epoch 1/5 - Sample 5000/40000 - Loss: 0.4942\n",
      "  Epoch 1/5 - Sample 6000/40000 - Loss: 0.3156\n",
      "  Epoch 1/5 - Sample 7000/40000 - Loss: 0.3154\n",
      "  Epoch 1/5 - Sample 8000/40000 - Loss: 0.3459\n",
      "  Epoch 1/5 - Sample 9000/40000 - Loss: 0.3135\n",
      "  Epoch 1/5 - Sample 10000/40000 - Loss: 0.3472\n",
      "  Epoch 1/5 - Sample 11000/40000 - Loss: 0.3182\n",
      "  Epoch 1/5 - Sample 12000/40000 - Loss: 0.4932\n",
      "  Epoch 1/5 - Sample 13000/40000 - Loss: 0.3153\n",
      "  Epoch 1/5 - Sample 14000/40000 - Loss: 0.3134\n",
      "  Epoch 1/5 - Sample 15000/40000 - Loss: 0.3137\n",
      "  Epoch 1/5 - Sample 16000/40000 - Loss: 0.3138\n",
      "  Epoch 1/5 - Sample 17000/40000 - Loss: 0.3152\n",
      "  Epoch 1/5 - Sample 18000/40000 - Loss: 0.3314\n",
      "  Epoch 1/5 - Sample 19000/40000 - Loss: 0.3136\n",
      "  Epoch 1/5 - Sample 20000/40000 - Loss: 0.4878\n",
      "  Epoch 1/5 - Sample 21000/40000 - Loss: 0.3145\n",
      "  Epoch 1/5 - Sample 22000/40000 - Loss: 0.3133\n",
      "  Epoch 1/5 - Sample 23000/40000 - Loss: 0.3144\n",
      "  Epoch 1/5 - Sample 24000/40000 - Loss: 0.3136\n",
      "  Epoch 1/5 - Sample 25000/40000 - Loss: 0.3311\n",
      "  Epoch 1/5 - Sample 26000/40000 - Loss: 0.3253\n",
      "  Epoch 1/5 - Sample 27000/40000 - Loss: 0.3321\n",
      "  Epoch 1/5 - Sample 28000/40000 - Loss: 0.3139\n",
      "  Epoch 1/5 - Sample 29000/40000 - Loss: 0.3176\n",
      "  Epoch 1/5 - Sample 30000/40000 - Loss: 0.3136\n",
      "  Epoch 1/5 - Sample 31000/40000 - Loss: 0.4657\n",
      "  Epoch 1/5 - Sample 32000/40000 - Loss: 0.3678\n",
      "  Epoch 1/5 - Sample 33000/40000 - Loss: 0.3138\n",
      "  Epoch 1/5 - Sample 34000/40000 - Loss: 0.3203\n",
      "  Epoch 1/5 - Sample 35000/40000 - Loss: 0.3151\n",
      "  Epoch 1/5 - Sample 36000/40000 - Loss: 0.3258\n",
      "  Epoch 1/5 - Sample 37000/40000 - Loss: 0.3133\n",
      "  Epoch 1/5 - Sample 38000/40000 - Loss: 0.3148\n",
      "  Epoch 1/5 - Sample 39000/40000 - Loss: 0.3145\n",
      "  Epoch 1/5 - Sample 40000/40000 - Loss: 0.3134\n",
      "‚úì Epoch 1/5 Complete - Avg Loss: 0.3451, Accuracy: 0.9767\n",
      "----------------------------------------------------------------------\n",
      "  Epoch 2/5 - Sample 1000/40000 - Loss: 0.3151\n",
      "  Epoch 2/5 - Sample 2000/40000 - Loss: 0.3326\n",
      "  Epoch 2/5 - Sample 3000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 4000/40000 - Loss: 0.3150\n",
      "  Epoch 2/5 - Sample 5000/40000 - Loss: 0.3453\n",
      "  Epoch 2/5 - Sample 6000/40000 - Loss: 0.3148\n",
      "  Epoch 2/5 - Sample 7000/40000 - Loss: 0.3228\n",
      "  Epoch 2/5 - Sample 8000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 9000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 10000/40000 - Loss: 0.3141\n",
      "  Epoch 2/5 - Sample 11000/40000 - Loss: 0.3137\n",
      "  Epoch 2/5 - Sample 12000/40000 - Loss: 0.3209\n",
      "  Epoch 2/5 - Sample 13000/40000 - Loss: 0.3135\n",
      "  Epoch 2/5 - Sample 14000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 15000/40000 - Loss: 0.3214\n",
      "  Epoch 2/5 - Sample 16000/40000 - Loss: 0.3141\n",
      "  Epoch 2/5 - Sample 17000/40000 - Loss: 0.3160\n",
      "  Epoch 2/5 - Sample 18000/40000 - Loss: 0.3138\n",
      "  Epoch 2/5 - Sample 19000/40000 - Loss: 0.3135\n",
      "  Epoch 2/5 - Sample 20000/40000 - Loss: 0.3168\n",
      "  Epoch 2/5 - Sample 21000/40000 - Loss: 0.3267\n",
      "  Epoch 2/5 - Sample 22000/40000 - Loss: 0.3143\n",
      "  Epoch 2/5 - Sample 23000/40000 - Loss: 0.4010\n",
      "  Epoch 2/5 - Sample 24000/40000 - Loss: 0.3333\n",
      "  Epoch 2/5 - Sample 25000/40000 - Loss: 0.3139\n",
      "  Epoch 2/5 - Sample 26000/40000 - Loss: 0.3149\n",
      "  Epoch 2/5 - Sample 27000/40000 - Loss: 0.4456\n",
      "  Epoch 2/5 - Sample 28000/40000 - Loss: 0.3169\n",
      "  Epoch 2/5 - Sample 29000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 30000/40000 - Loss: 0.5789\n",
      "  Epoch 2/5 - Sample 31000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 32000/40000 - Loss: 0.3135\n",
      "  Epoch 2/5 - Sample 33000/40000 - Loss: 0.3134\n",
      "  Epoch 2/5 - Sample 34000/40000 - Loss: 0.3135\n",
      "  Epoch 2/5 - Sample 35000/40000 - Loss: 0.3652\n",
      "  Epoch 2/5 - Sample 36000/40000 - Loss: 0.3156\n",
      "  Epoch 2/5 - Sample 37000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 38000/40000 - Loss: 0.3411\n",
      "  Epoch 2/5 - Sample 39000/40000 - Loss: 0.3133\n",
      "  Epoch 2/5 - Sample 40000/40000 - Loss: 0.3144\n",
      "‚úì Epoch 2/5 Complete - Avg Loss: 0.3448, Accuracy: 0.9767\n",
      "----------------------------------------------------------------------\n",
      "  Epoch 3/5 - Sample 1000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 2000/40000 - Loss: 0.3164\n",
      "  Epoch 3/5 - Sample 3000/40000 - Loss: 0.3154\n",
      "  Epoch 3/5 - Sample 4000/40000 - Loss: 0.3145\n",
      "  Epoch 3/5 - Sample 5000/40000 - Loss: 1.2264\n",
      "  Epoch 3/5 - Sample 6000/40000 - Loss: 0.3181\n",
      "  Epoch 3/5 - Sample 7000/40000 - Loss: 0.3146\n",
      "  Epoch 3/5 - Sample 8000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 9000/40000 - Loss: 0.3361\n",
      "  Epoch 3/5 - Sample 10000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 11000/40000 - Loss: 0.4050\n",
      "  Epoch 3/5 - Sample 12000/40000 - Loss: 0.3310\n",
      "  Epoch 3/5 - Sample 13000/40000 - Loss: 0.3195\n",
      "  Epoch 3/5 - Sample 14000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 15000/40000 - Loss: 0.3317\n",
      "  Epoch 3/5 - Sample 16000/40000 - Loss: 0.3206\n",
      "  Epoch 3/5 - Sample 17000/40000 - Loss: 0.3441\n",
      "  Epoch 3/5 - Sample 18000/40000 - Loss: 0.3186\n",
      "  Epoch 3/5 - Sample 19000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 20000/40000 - Loss: 0.3459\n",
      "  Epoch 3/5 - Sample 21000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 22000/40000 - Loss: 0.3193\n",
      "  Epoch 3/5 - Sample 23000/40000 - Loss: 0.3161\n",
      "  Epoch 3/5 - Sample 24000/40000 - Loss: 0.3145\n",
      "  Epoch 3/5 - Sample 25000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 26000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 27000/40000 - Loss: 0.3289\n",
      "  Epoch 3/5 - Sample 28000/40000 - Loss: 0.5395\n",
      "  Epoch 3/5 - Sample 29000/40000 - Loss: 0.3231\n",
      "  Epoch 3/5 - Sample 30000/40000 - Loss: 0.6860\n",
      "  Epoch 3/5 - Sample 31000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 32000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 33000/40000 - Loss: 0.3140\n",
      "  Epoch 3/5 - Sample 34000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 35000/40000 - Loss: 0.3133\n",
      "  Epoch 3/5 - Sample 36000/40000 - Loss: 0.3535\n",
      "  Epoch 3/5 - Sample 37000/40000 - Loss: 0.3135\n",
      "  Epoch 3/5 - Sample 38000/40000 - Loss: 0.3233\n",
      "  Epoch 3/5 - Sample 39000/40000 - Loss: 0.3167\n",
      "  Epoch 3/5 - Sample 40000/40000 - Loss: 0.7270\n",
      "‚úì Epoch 3/5 Complete - Avg Loss: 0.3448, Accuracy: 0.9767\n",
      "----------------------------------------------------------------------\n",
      "  Epoch 4/5 - Sample 1000/40000 - Loss: 0.3133\n",
      "  Epoch 4/5 - Sample 2000/40000 - Loss: 0.3237\n",
      "  Epoch 4/5 - Sample 3000/40000 - Loss: 0.3167\n",
      "  Epoch 4/5 - Sample 4000/40000 - Loss: 0.3175\n",
      "  Epoch 4/5 - Sample 5000/40000 - Loss: 0.3154\n",
      "  Epoch 4/5 - Sample 6000/40000 - Loss: 0.3150\n",
      "  Epoch 4/5 - Sample 7000/40000 - Loss: 0.3328\n",
      "  Epoch 4/5 - Sample 8000/40000 - Loss: 0.3600\n",
      "  Epoch 4/5 - Sample 9000/40000 - Loss: 0.3323\n",
      "  Epoch 4/5 - Sample 10000/40000 - Loss: 0.3137\n",
      "  Epoch 4/5 - Sample 11000/40000 - Loss: 0.3141\n",
      "  Epoch 4/5 - Sample 12000/40000 - Loss: 0.3135\n",
      "  Epoch 4/5 - Sample 13000/40000 - Loss: 0.3148\n",
      "  Epoch 4/5 - Sample 14000/40000 - Loss: 0.3134\n",
      "  Epoch 4/5 - Sample 15000/40000 - Loss: 0.3168\n",
      "  Epoch 4/5 - Sample 16000/40000 - Loss: 0.3283\n",
      "  Epoch 4/5 - Sample 17000/40000 - Loss: 0.3135\n",
      "  Epoch 4/5 - Sample 18000/40000 - Loss: 0.3189\n",
      "  Epoch 4/5 - Sample 19000/40000 - Loss: 0.3156\n",
      "  Epoch 4/5 - Sample 20000/40000 - Loss: 0.3144\n",
      "  Epoch 4/5 - Sample 21000/40000 - Loss: 0.3153\n",
      "  Epoch 4/5 - Sample 22000/40000 - Loss: 0.3283\n",
      "  Epoch 4/5 - Sample 23000/40000 - Loss: 0.3133\n",
      "  Epoch 4/5 - Sample 24000/40000 - Loss: 0.3136\n",
      "  Epoch 4/5 - Sample 25000/40000 - Loss: 0.3495\n",
      "  Epoch 4/5 - Sample 26000/40000 - Loss: 0.3198\n",
      "  Epoch 4/5 - Sample 27000/40000 - Loss: 0.3174\n",
      "  Epoch 4/5 - Sample 28000/40000 - Loss: 0.3139\n",
      "  Epoch 4/5 - Sample 29000/40000 - Loss: 0.3136\n",
      "  Epoch 4/5 - Sample 30000/40000 - Loss: 0.3258\n",
      "  Epoch 4/5 - Sample 31000/40000 - Loss: 0.3135\n",
      "  Epoch 4/5 - Sample 32000/40000 - Loss: 0.3186\n",
      "  Epoch 4/5 - Sample 33000/40000 - Loss: 0.4822\n",
      "  Epoch 4/5 - Sample 34000/40000 - Loss: 0.3133\n",
      "  Epoch 4/5 - Sample 35000/40000 - Loss: 0.3196\n",
      "  Epoch 4/5 - Sample 36000/40000 - Loss: 0.4173\n",
      "  Epoch 4/5 - Sample 37000/40000 - Loss: 0.3137\n",
      "  Epoch 4/5 - Sample 38000/40000 - Loss: 0.3139\n",
      "  Epoch 4/5 - Sample 39000/40000 - Loss: 0.3147\n",
      "  Epoch 4/5 - Sample 40000/40000 - Loss: 0.5013\n",
      "‚úì Epoch 4/5 Complete - Avg Loss: 0.3448, Accuracy: 0.9767\n",
      "----------------------------------------------------------------------\n",
      "  Epoch 5/5 - Sample 1000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 2000/40000 - Loss: 0.3236\n",
      "  Epoch 5/5 - Sample 3000/40000 - Loss: 0.3145\n",
      "  Epoch 5/5 - Sample 4000/40000 - Loss: 0.3208\n",
      "  Epoch 5/5 - Sample 5000/40000 - Loss: 0.3150\n",
      "  Epoch 5/5 - Sample 6000/40000 - Loss: 0.3157\n",
      "  Epoch 5/5 - Sample 7000/40000 - Loss: 0.3513\n",
      "  Epoch 5/5 - Sample 8000/40000 - Loss: 0.3176\n",
      "  Epoch 5/5 - Sample 9000/40000 - Loss: 0.3247\n",
      "  Epoch 5/5 - Sample 10000/40000 - Loss: 0.3151\n",
      "  Epoch 5/5 - Sample 11000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 12000/40000 - Loss: 0.3285\n",
      "  Epoch 5/5 - Sample 13000/40000 - Loss: 0.6258\n",
      "  Epoch 5/5 - Sample 14000/40000 - Loss: 0.3135\n",
      "  Epoch 5/5 - Sample 15000/40000 - Loss: 0.3216\n",
      "  Epoch 5/5 - Sample 16000/40000 - Loss: 0.3136\n",
      "  Epoch 5/5 - Sample 17000/40000 - Loss: 0.3135\n",
      "  Epoch 5/5 - Sample 18000/40000 - Loss: 0.3302\n",
      "  Epoch 5/5 - Sample 19000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 20000/40000 - Loss: 0.3139\n",
      "  Epoch 5/5 - Sample 21000/40000 - Loss: 0.3226\n",
      "  Epoch 5/5 - Sample 22000/40000 - Loss: 0.3138\n",
      "  Epoch 5/5 - Sample 23000/40000 - Loss: 0.3171\n",
      "  Epoch 5/5 - Sample 24000/40000 - Loss: 0.3146\n",
      "  Epoch 5/5 - Sample 25000/40000 - Loss: 0.3409\n",
      "  Epoch 5/5 - Sample 26000/40000 - Loss: 0.3354\n",
      "  Epoch 5/5 - Sample 27000/40000 - Loss: 0.3153\n",
      "  Epoch 5/5 - Sample 28000/40000 - Loss: 0.5105\n",
      "  Epoch 5/5 - Sample 29000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 30000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 31000/40000 - Loss: 1.1735\n",
      "  Epoch 5/5 - Sample 32000/40000 - Loss: 0.3279\n",
      "  Epoch 5/5 - Sample 33000/40000 - Loss: 0.3134\n",
      "  Epoch 5/5 - Sample 34000/40000 - Loss: 0.3992\n",
      "  Epoch 5/5 - Sample 35000/40000 - Loss: 0.3200\n",
      "  Epoch 5/5 - Sample 36000/40000 - Loss: 0.3468\n",
      "  Epoch 5/5 - Sample 37000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 38000/40000 - Loss: 0.3133\n",
      "  Epoch 5/5 - Sample 39000/40000 - Loss: 0.7502\n",
      "  Epoch 5/5 - Sample 40000/40000 - Loss: 0.3231\n",
      "‚úì Epoch 5/5 Complete - Avg Loss: 0.3448, Accuracy: 0.9767\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ Gating Network Training Complete!\n",
      "\n",
      "Evaluating Mixture of Experts...\n",
      "\n",
      "======================================================================\n",
      "üìä EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "TN:   6342  |  FP:    147\n",
      "FN:     73  |  TP:   3438\n",
      "\n",
      "=== Performance Metrics ===\n",
      "Accuracy:  0.9780\n",
      "Precision: 0.9590\n",
      "Recall:    0.9792\n",
      "F1-Score:  0.9690\n",
      "ROC-AUC:   0.6093\n",
      "PR-AUC:    0.5431\n",
      "======================================================================\n",
      "\n",
      "Saving gating network...\n",
      "‚úÖ Gating network saved as 'gating_network.pth'\n",
      "\n",
      "\n",
      "==========================üéâ READY TO TEST! ===========================\n",
      "\n",
      "Use test_sample() to test with the TRAINED gating network:\n",
      "\n",
      "test_sample(\"URGENT! Click here http://paypa1.com\")\n",
      "test_sample(\"Hey, want to grab coffee?\")\n",
      "test_sample(\"http://suspicious-site.com\")\n",
      "\n",
      "The gating network now LEARNS which expert to trust! üß†\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Define URLFeatures Class\n",
    "# ============================================================================\n",
    "\n",
    "class URLFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, urls):\n",
    "        urls = np.array(urls).reshape(-1)\n",
    "        feats = np.array([\n",
    "            [\n",
    "                len(u),\n",
    "                u.count('-'),\n",
    "                u.count('@'),\n",
    "                u.count('?'),\n",
    "                u.count('='),\n",
    "                u.count('.'),\n",
    "                int(u.startswith(\"https\")),\n",
    "                int(u.count(\"//\") > 1)\n",
    "            ]\n",
    "            for u in urls\n",
    "        ])\n",
    "        return csr_matrix(feats)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Load Expert Models\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading Expert Models...\")\n",
    "\n",
    "URL_MODEL_PATH = r\"C:\\Users\\angelo\\Downloads\\THESIS\\URL_Expert-20251210T060216Z-1-001\\URL_Expert\\Notebook and Model\\url_expert_1.pkl\"\n",
    "expert_1 = joblib.load(URL_MODEL_PATH)\n",
    "print(\"‚úì Expert 1 (URL) loaded\")\n",
    "\n",
    "TEXT_MODEL_PATH = r\"C:\\Users\\angelo\\Downloads\\THESIS\\distilbert_phishing_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "expert_2 = AutoModelForSequenceClassification.from_pretrained(TEXT_MODEL_PATH)\n",
    "expert_2.eval()\n",
    "print(\"‚úì Expert 2 (Text) loaded\")\n",
    "\n",
    "print(\"\\n‚úÖ Both expert models loaded!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Load and Prepare Training Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load URL dataset\n",
    "df_url = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\url_dataset_combined_1.csv\")\n",
    "df_url = df_url[['url', 'label']].copy()\n",
    "df_url['text'] = \"\"  # Add empty text column\n",
    "\n",
    "# Load text dataset (with encoding fix)\n",
    "try:\n",
    "    # Try UTF-8 first\n",
    "    df_text = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\Dataset_100k.csv\")\n",
    "except UnicodeDecodeError:\n",
    "    # If UTF-8 fails, try latin-1 encoding\n",
    "    print(\"UTF-8 failed, trying latin-1 encoding...\")\n",
    "    df_text = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\Dataset_100k.csv\", encoding='latin-1')\n",
    "except:\n",
    "    # Last resort: try ISO-8859-1\n",
    "    print(\"Trying ISO-8859-1 encoding...\")\n",
    "    df_text = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\Dataset_100k.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "df_text = df_text[['label', 'text']].copy()\n",
    "df_text['url'] = \"\"  # Add empty url column\n",
    "\n",
    "print(f\"URL dataset: {len(df_url)} samples\")\n",
    "print(f\"Text dataset: {len(df_text)} samples\")\n",
    "\n",
    "# Combine datasets\n",
    "df_combined = pd.concat([df_url, df_text], ignore_index=True)\n",
    "df_combined = df_combined.dropna()\n",
    "\n",
    "# Use 50k samples - sweet spot between speed and accuracy\n",
    "df_combined = df_combined.sample(n=min(50000, len(df_combined)), random_state=42)\n",
    "\n",
    "print(f\"Combined dataset: {len(df_combined)} samples\")\n",
    "print(f\"Label distribution:\\n{df_combined['label'].value_counts()}\\n\")\n",
    "\n",
    "# Split into train and test\n",
    "X = df_combined[['text', 'url']]\n",
    "y = df_combined['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def calculate_phrase_score(text, phrase_dict):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    text_lower = text.lower()\n",
    "    score = 0.0\n",
    "    for phrase, weight in phrase_dict.items():\n",
    "        if phrase in text_lower:\n",
    "            score += weight\n",
    "    return min(score, 1.0)\n",
    "\n",
    "def extract_gating_features(text, url, phrase_score):\n",
    "    url_present = 1 if (url and not pd.isna(url) and url != \"\") else 0\n",
    "    message_length = len(text.split()) if text else 0\n",
    "    emoji_count = len(re.findall(r'[^\\w\\s,]', text)) if text else 0\n",
    "    hashtag_count = text.count('#') if text else 0\n",
    "    url_count = len(re.findall(r'http\\S+', text)) if text else 0\n",
    "    \n",
    "    if text and len(text) > 0:\n",
    "        capital_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
    "    else:\n",
    "        capital_ratio = 0.0\n",
    "    \n",
    "    embedding_summary = 0.0  # Placeholder\n",
    "    \n",
    "    features = np.array([\n",
    "        url_present,\n",
    "        phrase_score,\n",
    "        message_length,\n",
    "        emoji_count,\n",
    "        hashtag_count,\n",
    "        url_count,\n",
    "        capital_ratio,\n",
    "        embedding_summary\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return features\n",
    "\n",
    "phrase_dict = {\n",
    "    'urgent': 0.3,\n",
    "    'verify account': 0.5,\n",
    "    'suspended': 0.4,\n",
    "    'click here': 0.3,\n",
    "    'confirm your': 0.4,\n",
    "    'congratulations': 0.3,\n",
    "    'winner': 0.4,\n",
    "    'limited time': 0.3,\n",
    "    'act now': 0.3,\n",
    "    'security alert': 0.5,\n",
    "    'claim': 0.3,\n",
    "    'prize': 0.3,\n",
    "    'free': 0.2,\n",
    "    'bonus': 0.2,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Define Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=64, num_experts=2):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        weights = self.softmax(x)\n",
    "        return weights\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Train Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "def train_gating_network(X_train, y_train, expert_1, expert_2, phrase_dict, num_epochs=5):\n",
    "    gating_net = GatingNetwork(input_size=8, hidden_size=64, num_experts=2)\n",
    "    optimizer = optim.Adam(gating_net.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    gating_net.train()\n",
    "    \n",
    "    print(\"Training Gating Network...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Shuffle data each epoch\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            text = X_train.iloc[idx]['text']\n",
    "            url = X_train.iloc[idx]['url']\n",
    "            label = y_train.iloc[idx]\n",
    "            \n",
    "            text = preprocess_text(text)\n",
    "            phrase_score = calculate_phrase_score(text, phrase_dict)\n",
    "            \n",
    "            # Get URL expert prediction\n",
    "            if url and not pd.isna(url) and url != \"\":\n",
    "                try:\n",
    "                    url_df = pd.DataFrame({'url': [url]})\n",
    "                    url_probs = expert_1.predict_proba(url_df)[0]\n",
    "                except:\n",
    "                    url_probs = np.array([0.5, 0.5])\n",
    "            else:\n",
    "                url_probs = np.array([0.5, 0.5])\n",
    "            \n",
    "            # Get text expert prediction\n",
    "            if text:\n",
    "                try:\n",
    "                    inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                                     truncation=True, max_length=128)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = expert_2(**inputs)\n",
    "                        text_probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
    "                except:\n",
    "                    text_probs = np.array([0.5, 0.5])\n",
    "            else:\n",
    "                text_probs = np.array([0.5, 0.5])\n",
    "            \n",
    "            # Convert expert predictions to tensors (no gradient needed)\n",
    "            url_probs_tensor = torch.FloatTensor(url_probs).unsqueeze(0)\n",
    "            text_probs_tensor = torch.FloatTensor(text_probs).unsqueeze(0)\n",
    "            \n",
    "            # Get gating features\n",
    "            gating_features = extract_gating_features(text, url, phrase_score)\n",
    "            gating_input = torch.FloatTensor(gating_features).unsqueeze(0)\n",
    "            \n",
    "            # Get expert weights (requires grad)\n",
    "            expert_weights = gating_net(gating_input)\n",
    "            \n",
    "            # Combine predictions using matrix multiplication to preserve gradients\n",
    "            # Stack expert predictions [batch, num_experts, num_classes]\n",
    "            expert_preds = torch.stack([url_probs_tensor, text_probs_tensor], dim=1)\n",
    "            \n",
    "            # Expand weights for multiplication [batch, num_experts, 1]\n",
    "            weights_expanded = expert_weights.unsqueeze(2)\n",
    "            \n",
    "            # Weighted sum: [batch, num_classes]\n",
    "            final_probs = (expert_preds * weights_expanded).sum(dim=1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            label_tensor = torch.LongTensor([label])\n",
    "            loss = criterion(final_probs, label_tensor)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            pred = torch.argmax(final_probs, dim=1)\n",
    "            correct += (pred == label_tensor).sum().item()\n",
    "            total += 1\n",
    "            \n",
    "            # Print progress every 1000 samples\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{num_epochs} - Sample {i+1}/{len(X_train)} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        accuracy = correct / total\n",
    "        print(f\"‚úì Epoch {epoch+1}/{num_epochs} Complete - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    print(\"\\n‚úÖ Gating Network Training Complete!\\n\")\n",
    "    return gating_net\n",
    "\n",
    "# Train the gating network\n",
    "gating_net = train_gating_network(X_train, y_train, expert_1, expert_2, phrase_dict, num_epochs=5)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_moe(X_test, y_test, expert_1, expert_2, gating_net, phrase_dict):\n",
    "    print(\"Evaluating Mixture of Experts...\")\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    gating_net.eval()\n",
    "    \n",
    "    for idx in range(len(X_test)):\n",
    "        text = X_test.iloc[idx]['text']\n",
    "        url = X_test.iloc[idx]['url']\n",
    "        \n",
    "        text = preprocess_text(text)\n",
    "        phrase_score = calculate_phrase_score(text, phrase_dict)\n",
    "        \n",
    "        # Get predictions\n",
    "        if url and not pd.isna(url) and url != \"\":\n",
    "            try:\n",
    "                url_df = pd.DataFrame({'url': [url]})\n",
    "                url_probs = expert_1.predict_proba(url_df)[0]\n",
    "            except:\n",
    "                url_probs = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            url_probs = np.array([0.5, 0.5])\n",
    "        \n",
    "        if text:\n",
    "            try:\n",
    "                inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                                 truncation=True, max_length=128)\n",
    "                with torch.no_grad():\n",
    "                    outputs = expert_2(**inputs)\n",
    "                    text_probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
    "            except:\n",
    "                text_probs = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            text_probs = np.array([0.5, 0.5])\n",
    "        \n",
    "        gating_features = extract_gating_features(text, url, phrase_score)\n",
    "        gating_input = torch.FloatTensor(gating_features).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            expert_weights = gating_net(gating_input)\n",
    "        \n",
    "        final_probs = (expert_weights[0, 0].item() * url_probs + \n",
    "                      expert_weights[0, 1].item() * text_probs)\n",
    "        \n",
    "        predictions.append(1 if final_probs[1] > 0.5 else 0)\n",
    "        confidences.append(max(final_probs))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, confidences)\n",
    "    pr_auc = average_precision_score(y_test, confidences)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    print(f\"TN: {cm[0,0]:6}  |  FP: {cm[0,1]:6}\")\n",
    "    print(f\"FN: {cm[1,0]:6}  |  TP: {cm[1,1]:6}\")\n",
    "    \n",
    "    print(\"\\n=== Performance Metrics ===\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return predictions, confidences\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_moe(X_test, y_test, expert_1, expert_2, gating_net, phrase_dict)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Save Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nSaving gating network...\")\n",
    "torch.save(gating_net.state_dict(), 'gating_network.pth')\n",
    "print(\"‚úÖ Gating network saved as 'gating_network.pth'\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: Prediction Function with Trained Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "def predict_with_gating(text, url, gating_net, expert_1, expert_2, phrase_dict):\n",
    "    text = preprocess_text(text)\n",
    "    phrase_score = calculate_phrase_score(text, phrase_dict)\n",
    "    \n",
    "    # Get predictions\n",
    "    if url and url.strip():\n",
    "        try:\n",
    "            url_df = pd.DataFrame({'url': [url]})\n",
    "            url_probs = expert_1.predict_proba(url_df)[0]\n",
    "        except:\n",
    "            url_probs = np.array([0.5, 0.5])\n",
    "    else:\n",
    "        url_probs = np.array([0.5, 0.5])\n",
    "    \n",
    "    if text:\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                             truncation=True, max_length=128)\n",
    "            with torch.no_grad():\n",
    "                outputs = expert_2(**inputs)\n",
    "                text_probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
    "        except:\n",
    "            text_probs = np.array([0.5, 0.5])\n",
    "    else:\n",
    "        text_probs = np.array([0.5, 0.5])\n",
    "    \n",
    "    gating_features = extract_gating_features(text, url, phrase_score)\n",
    "    gating_input = torch.FloatTensor(gating_features).unsqueeze(0)\n",
    "    \n",
    "    gating_net.eval()\n",
    "    with torch.no_grad():\n",
    "        expert_weights = gating_net(gating_input)\n",
    "    \n",
    "    final_probs = (expert_weights[0, 0].item() * url_probs + \n",
    "                  expert_weights[0, 1].item() * text_probs)\n",
    "    \n",
    "    prediction = \"PHISHING ‚ö†Ô∏è\" if final_probs[1] > 0.5 else \"SAFE ‚úÖ\"\n",
    "    confidence = max(final_probs) * 100\n",
    "    \n",
    "    url_contrib = expert_weights[0, 0].item() * 100\n",
    "    text_contrib = expert_weights[0, 1].item() * 100\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence,\n",
    "        'url_weight': url_contrib,\n",
    "        'text_weight': text_contrib,\n",
    "        'url_prediction': 'PHISHING' if url_probs[1] > 0.5 else 'SAFE',\n",
    "        'text_prediction': 'PHISHING' if text_probs[1] > 0.5 else 'SAFE',\n",
    "        'phrase_score': phrase_score\n",
    "    }\n",
    "\n",
    "def test_sample(input_text):\n",
    "    \"\"\"Smart auto-detection for testing\"\"\"\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    urls = re.findall(url_pattern, input_text)\n",
    "    \n",
    "    if urls:\n",
    "        url = urls[0]\n",
    "        text = re.sub(url_pattern, '', input_text).strip()\n",
    "    else:\n",
    "        url = \"\"\n",
    "        text = input_text.strip()\n",
    "    \n",
    "    results = predict_with_gating(text, url, gating_net, expert_1, expert_2, phrase_dict)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéØ PREDICTION RESULTS (with Trained Gating Network)\")\n",
    "    print(\"=\" * 70)\n",
    "    if text:\n",
    "        print(f\"üìù Text: {text[:80]}...\" if len(text) > 80 else f\"üìù Text: {text}\")\n",
    "    if url:\n",
    "        print(f\"üîó URL: {url}\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"üß† Gating Network Weights:\")\n",
    "    print(f\"  üåê URL Expert:  {results['url_weight']:.1f}%\")\n",
    "    print(f\"  üìÑ Text Expert: {results['text_weight']:.1f}%\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"üéØ FINAL: {results['prediction']}\")\n",
    "    print(f\"üìä Confidence: {results['confidence']:.2f}%\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"üéâ READY TO TEST! \".center(70, \"=\"))\n",
    "print(\"\"\"\n",
    "Use test_sample() to test with the TRAINED gating network:\n",
    "\n",
    "test_sample(\"URGENT! Click here http://paypa1.com\")\n",
    "test_sample(\"Hey, want to grab coffee?\")\n",
    "test_sample(\"http://suspicious-site.com\")\n",
    "\n",
    "The gating network now LEARNS which expert to trust! üß†\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5254edd2-393d-4e37-bfd8-d19d73ce064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving complete MoE system...\n",
      "‚úì Gating network saved\n",
      "‚úì Phrase dictionary saved\n",
      "‚úì Expert models already saved at original locations\n",
      "\n",
      "‚úÖ Complete MoE system saved!\n",
      "\n",
      "Saved files:\n",
      "  - gating_network.pth (trained gating network)\n",
      "  - phrase_dict.pkl (phrase scoring dictionary)\n",
      "  - C:\\Users\\angelo\\Downloads\\THESIS\\URL_Expert-20251210T060216Z-1-001\\URL_Expert\\Notebook and Model\\url_expert_1.pkl (URL expert)\n",
      "  - C:\\Users\\angelo\\Downloads\\THESIS\\distilbert_phishing_model (Text expert)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE MODEL SAVING\n",
    "# ============================================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"Saving complete MoE system...\")\n",
    "\n",
    "# 1. Save Gating Network (you already have this)\n",
    "torch.save(gating_net.state_dict(), 'gating_network.pth')\n",
    "print(\"‚úì Gating network saved\")\n",
    "\n",
    "# 2. Save phrase dictionary\n",
    "with open('phrase_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(phrase_dict, f)\n",
    "print(\"‚úì Phrase dictionary saved\")\n",
    "\n",
    "# 3. Expert models are already saved at their original locations:\n",
    "# - expert_1 (URL): Already at URL_MODEL_PATH\n",
    "# - expert_2 (Text): Already at TEXT_MODEL_PATH\n",
    "print(\"‚úì Expert models already saved at original locations\")\n",
    "\n",
    "print(\"\\n‚úÖ Complete MoE system saved!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - gating_network.pth (trained gating network)\")\n",
    "print(\"  - phrase_dict.pkl (phrase scoring dictionary)\")\n",
    "print(f\"  - {URL_MODEL_PATH} (URL expert)\")\n",
    "print(f\"  - {TEXT_MODEL_PATH} (Text expert)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d40cf3c-2e12-4a63-861a-83e8a0619341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOADING COMPLETE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def load_moe_system():\n",
    "    \"\"\"Load complete trained MoE system\"\"\"\n",
    "    \n",
    "    # 1. Load expert models\n",
    "    expert_1 = joblib.load(r\"C:\\Users\\angelo\\Downloads\\THESIS\\URL_Expert-20251210T060216Z-1-001\\URL_Expert\\Notebook and Model\\url_expert_1.pkl\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(r\"C:\\Users\\angelo\\Downloads\\THESIS\\distilbert_phishing_model\")\n",
    "    expert_2 = AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\angelo\\Downloads\\THESIS\\distilbert_phishing_model\")\n",
    "    expert_2.eval()\n",
    "    \n",
    "    # 2. Load gating network\n",
    "    gating_net = GatingNetwork(input_size=8, hidden_size=64, num_experts=2)\n",
    "    gating_net.load_state_dict(torch.load('gating_network.pth'))\n",
    "    gating_net.eval()\n",
    "    \n",
    "    # 3. Load phrase dictionary\n",
    "    with open('phrase_dict.pkl', 'rb') as f:\n",
    "        phrase_dict = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Complete MoE system loaded!\")\n",
    "    \n",
    "    return expert_1, expert_2, tokenizer, gating_net, phrase_dict\n",
    "\n",
    "# Usage:\n",
    "# expert_1, expert_2, tokenizer, gating_net, phrase_dict = load_moe_system()\n",
    "# Now you can use predict_with_gating() or test_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5c24f-f2bb-4194-b384-e54dce5cbac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
