{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5551c8-6ab4-4058-8e34-104452d8f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Expert Models...\n",
      "âœ“ Expert 1 (URL) loaded\n",
      "âœ“ Expert 2 (Text) loaded\n",
      "\n",
      "âœ… Both expert models loaded!\n",
      "\n",
      "Loading datasets...\n",
      "UTF-8 failed, trying latin-1 encoding...\n",
      "URL dataset: 722802 samples\n",
      "Text dataset: 150355 samples\n",
      "Combined dataset: 20000 samples\n",
      "Label distribution:\n",
      "label\n",
      "0.0    12995\n",
      "1.0     7005\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: 16000 samples\n",
      "Test set: 4000 samples\n",
      "\n",
      "Training Gating Network...\n",
      "======================================================================\n",
      "  Epoch 1/5 - Sample 1000/16000 - Loss: 0.3133\n",
      "  Epoch 1/5 - Sample 2000/16000 - Loss: 0.3243\n",
      "  Epoch 1/5 - Sample 3000/16000 - Loss: 0.6089\n",
      "  Epoch 1/5 - Sample 4000/16000 - Loss: 0.3174\n",
      "  Epoch 1/5 - Sample 5000/16000 - Loss: 0.3181\n",
      "  Epoch 1/5 - Sample 6000/16000 - Loss: 0.3147\n",
      "  Epoch 1/5 - Sample 7000/16000 - Loss: 0.3145\n",
      "  Epoch 1/5 - Sample 8000/16000 - Loss: 0.3232\n",
      "  Epoch 1/5 - Sample 9000/16000 - Loss: 0.3324\n",
      "  Epoch 1/5 - Sample 10000/16000 - Loss: 0.3818\n",
      "  Epoch 1/5 - Sample 11000/16000 - Loss: 0.3213\n",
      "  Epoch 1/5 - Sample 12000/16000 - Loss: 0.3629\n",
      "  Epoch 1/5 - Sample 13000/16000 - Loss: 0.3138\n",
      "  Epoch 1/5 - Sample 14000/16000 - Loss: 0.3650\n",
      "  Epoch 1/5 - Sample 15000/16000 - Loss: 0.3205\n",
      "  Epoch 1/5 - Sample 16000/16000 - Loss: 0.3133\n",
      "âœ“ Epoch 1/5 Complete - Avg Loss: 0.3464, Accuracy: 0.9766\n",
      "----------------------------------------------------------------------\n",
      "  Epoch 2/5 - Sample 1000/16000 - Loss: 0.3135\n",
      "  Epoch 2/5 - Sample 2000/16000 - Loss: 0.3148\n",
      "  Epoch 2/5 - Sample 3000/16000 - Loss: 0.3176\n",
      "  Epoch 2/5 - Sample 4000/16000 - Loss: 0.3149\n",
      "  Epoch 2/5 - Sample 5000/16000 - Loss: 0.3155\n",
      "  Epoch 2/5 - Sample 6000/16000 - Loss: 0.3153\n",
      "  Epoch 2/5 - Sample 7000/16000 - Loss: 0.3568\n",
      "  Epoch 2/5 - Sample 8000/16000 - Loss: 0.3137\n",
      "  Epoch 2/5 - Sample 9000/16000 - Loss: 0.3414\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Define URLFeatures Class\n",
    "# ============================================================================\n",
    "\n",
    "class URLFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, urls):\n",
    "        urls = np.array(urls).reshape(-1)\n",
    "        feats = np.array([\n",
    "            [\n",
    "                len(u),\n",
    "                u.count('-'),\n",
    "                u.count('@'),\n",
    "                u.count('?'),\n",
    "                u.count('='),\n",
    "                u.count('.'),\n",
    "                int(u.startswith(\"https\")),\n",
    "                int(u.count(\"//\") > 1)\n",
    "            ]\n",
    "            for u in urls\n",
    "        ])\n",
    "        return csr_matrix(feats)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Load Expert Models\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading Expert Models...\")\n",
    "\n",
    "URL_MODEL_PATH = r\"C:\\Users\\angelo\\Downloads\\THESIS\\URL_Expert-20251210T060216Z-1-001\\URL_Expert\\Notebook and Model\\url_expert_1.pkl\"\n",
    "expert_1 = joblib.load(URL_MODEL_PATH)\n",
    "print(\"âœ“ Expert 1 (URL) loaded\")\n",
    "\n",
    "TEXT_MODEL_PATH = r\"C:\\Users\\angelo\\Downloads\\THESIS\\distilbert_phishing_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "expert_2 = AutoModelForSequenceClassification.from_pretrained(TEXT_MODEL_PATH)\n",
    "expert_2.eval()\n",
    "print(\"âœ“ Expert 2 (Text) loaded\")\n",
    "\n",
    "print(\"\\nâœ… Both expert models loaded!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Load and Prepare Training Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load URL dataset\n",
    "df_url = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\url_dataset_combined_1.csv\")\n",
    "df_url = df_url[['url', 'label']].copy()\n",
    "df_url['text'] = \"\"  # Add empty text column\n",
    "\n",
    "# Load text dataset (with encoding fix)\n",
    "try:\n",
    "    # Try UTF-8 first\n",
    "    df_text = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\Dataset_100k.csv\")\n",
    "except UnicodeDecodeError:\n",
    "    # If UTF-8 fails, try latin-1 encoding\n",
    "    print(\"UTF-8 failed, trying latin-1 encoding...\")\n",
    "    df_text = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\Dataset_100k.csv\", encoding='latin-1')\n",
    "except:\n",
    "    # Last resort: try ISO-8859-1\n",
    "    print(\"Trying ISO-8859-1 encoding...\")\n",
    "    df_text = pd.read_csv(r\"C:\\Users\\angelo\\Downloads\\THESIS\\Dataset_5971\\Dataset_100k.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "df_text = df_text[['label', 'text']].copy()\n",
    "df_text['url'] = \"\"  # Add empty url column\n",
    "\n",
    "print(f\"URL dataset: {len(df_url)} samples\")\n",
    "print(f\"Text dataset: {len(df_text)} samples\")\n",
    "\n",
    "# Combine datasets\n",
    "df_combined = pd.concat([df_url, df_text], ignore_index=True)\n",
    "df_combined = df_combined.dropna()\n",
    "\n",
    "# Sample data to make training faster (optional - remove if you want to use all data)\n",
    "# Using 20k samples for faster training\n",
    "df_combined = df_combined.sample(n=min(20000, len(df_combined)), random_state=42)\n",
    "\n",
    "print(f\"Combined dataset: {len(df_combined)} samples\")\n",
    "print(f\"Label distribution:\\n{df_combined['label'].value_counts()}\\n\")\n",
    "\n",
    "# Split into train and test\n",
    "X = df_combined[['text', 'url']]\n",
    "y = df_combined['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def calculate_phrase_score(text, phrase_dict):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    text_lower = text.lower()\n",
    "    score = 0.0\n",
    "    for phrase, weight in phrase_dict.items():\n",
    "        if phrase in text_lower:\n",
    "            score += weight\n",
    "    return min(score, 1.0)\n",
    "\n",
    "def extract_gating_features(text, url, phrase_score):\n",
    "    url_present = 1 if (url and not pd.isna(url) and url != \"\") else 0\n",
    "    message_length = len(text.split()) if text else 0\n",
    "    emoji_count = len(re.findall(r'[^\\w\\s,]', text)) if text else 0\n",
    "    hashtag_count = text.count('#') if text else 0\n",
    "    url_count = len(re.findall(r'http\\S+', text)) if text else 0\n",
    "    \n",
    "    if text and len(text) > 0:\n",
    "        capital_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
    "    else:\n",
    "        capital_ratio = 0.0\n",
    "    \n",
    "    embedding_summary = 0.0  # Placeholder\n",
    "    \n",
    "    features = np.array([\n",
    "        url_present,\n",
    "        phrase_score,\n",
    "        message_length,\n",
    "        emoji_count,\n",
    "        hashtag_count,\n",
    "        url_count,\n",
    "        capital_ratio,\n",
    "        embedding_summary\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return features\n",
    "\n",
    "phrase_dict = {\n",
    "    'urgent': 0.3,\n",
    "    'verify account': 0.5,\n",
    "    'suspended': 0.4,\n",
    "    'click here': 0.3,\n",
    "    'confirm your': 0.4,\n",
    "    'congratulations': 0.3,\n",
    "    'winner': 0.4,\n",
    "    'limited time': 0.3,\n",
    "    'act now': 0.3,\n",
    "    'security alert': 0.5,\n",
    "    'claim': 0.3,\n",
    "    'prize': 0.3,\n",
    "    'free': 0.2,\n",
    "    'bonus': 0.2,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Define Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=64, num_experts=2):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        weights = self.softmax(x)\n",
    "        return weights\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Train Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "def train_gating_network(X_train, y_train, expert_1, expert_2, phrase_dict, num_epochs=5):\n",
    "    gating_net = GatingNetwork(input_size=8, hidden_size=64, num_experts=2)\n",
    "    optimizer = optim.Adam(gating_net.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    gating_net.train()\n",
    "    \n",
    "    print(\"Training Gating Network...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Shuffle data each epoch\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            text = X_train.iloc[idx]['text']\n",
    "            url = X_train.iloc[idx]['url']\n",
    "            label = y_train.iloc[idx]\n",
    "            \n",
    "            text = preprocess_text(text)\n",
    "            phrase_score = calculate_phrase_score(text, phrase_dict)\n",
    "            \n",
    "            # Get URL expert prediction\n",
    "            if url and not pd.isna(url) and url != \"\":\n",
    "                try:\n",
    "                    url_df = pd.DataFrame({'url': [url]})\n",
    "                    url_probs = expert_1.predict_proba(url_df)[0]\n",
    "                except:\n",
    "                    url_probs = np.array([0.5, 0.5])\n",
    "            else:\n",
    "                url_probs = np.array([0.5, 0.5])\n",
    "            \n",
    "            # Get text expert prediction\n",
    "            if text:\n",
    "                try:\n",
    "                    inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                                     truncation=True, max_length=128)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = expert_2(**inputs)\n",
    "                        text_probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
    "                except:\n",
    "                    text_probs = np.array([0.5, 0.5])\n",
    "            else:\n",
    "                text_probs = np.array([0.5, 0.5])\n",
    "            \n",
    "            # Convert expert predictions to tensors (no gradient needed)\n",
    "            url_probs_tensor = torch.FloatTensor(url_probs).unsqueeze(0)\n",
    "            text_probs_tensor = torch.FloatTensor(text_probs).unsqueeze(0)\n",
    "            \n",
    "            # Get gating features\n",
    "            gating_features = extract_gating_features(text, url, phrase_score)\n",
    "            gating_input = torch.FloatTensor(gating_features).unsqueeze(0)\n",
    "            \n",
    "            # Get expert weights (requires grad)\n",
    "            expert_weights = gating_net(gating_input)\n",
    "            \n",
    "            # Combine predictions using matrix multiplication to preserve gradients\n",
    "            # Stack expert predictions [batch, num_experts, num_classes]\n",
    "            expert_preds = torch.stack([url_probs_tensor, text_probs_tensor], dim=1)\n",
    "            \n",
    "            # Expand weights for multiplication [batch, num_experts, 1]\n",
    "            weights_expanded = expert_weights.unsqueeze(2)\n",
    "            \n",
    "            # Weighted sum: [batch, num_classes]\n",
    "            final_probs = (expert_preds * weights_expanded).sum(dim=1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            label_tensor = torch.LongTensor([label])\n",
    "            loss = criterion(final_probs, label_tensor)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            pred = torch.argmax(final_probs, dim=1)\n",
    "            correct += (pred == label_tensor).sum().item()\n",
    "            total += 1\n",
    "            \n",
    "            # Print progress every 1000 samples\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{num_epochs} - Sample {i+1}/{len(X_train)} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        accuracy = correct / total\n",
    "        print(f\"âœ“ Epoch {epoch+1}/{num_epochs} Complete - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    print(\"\\nâœ… Gating Network Training Complete!\\n\")\n",
    "    return gating_net\n",
    "\n",
    "# Train the gating network\n",
    "gating_net = train_gating_network(X_train, y_train, expert_1, expert_2, phrase_dict, num_epochs=5)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_moe(X_test, y_test, expert_1, expert_2, gating_net, phrase_dict):\n",
    "    print(\"Evaluating Mixture of Experts...\")\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    gating_net.eval()\n",
    "    \n",
    "    for idx in range(len(X_test)):\n",
    "        text = X_test.iloc[idx]['text']\n",
    "        url = X_test.iloc[idx]['url']\n",
    "        \n",
    "        text = preprocess_text(text)\n",
    "        phrase_score = calculate_phrase_score(text, phrase_dict)\n",
    "        \n",
    "        # Get predictions\n",
    "        if url and not pd.isna(url) and url != \"\":\n",
    "            try:\n",
    "                url_df = pd.DataFrame({'url': [url]})\n",
    "                url_probs = expert_1.predict_proba(url_df)[0]\n",
    "            except:\n",
    "                url_probs = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            url_probs = np.array([0.5, 0.5])\n",
    "        \n",
    "        if text:\n",
    "            try:\n",
    "                inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                                 truncation=True, max_length=128)\n",
    "                with torch.no_grad():\n",
    "                    outputs = expert_2(**inputs)\n",
    "                    text_probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
    "            except:\n",
    "                text_probs = np.array([0.5, 0.5])\n",
    "        else:\n",
    "            text_probs = np.array([0.5, 0.5])\n",
    "        \n",
    "        gating_features = extract_gating_features(text, url, phrase_score)\n",
    "        gating_input = torch.FloatTensor(gating_features).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            expert_weights = gating_net(gating_input)\n",
    "        \n",
    "        final_probs = (expert_weights[0, 0].item() * url_probs + \n",
    "                      expert_weights[0, 1].item() * text_probs)\n",
    "        \n",
    "        predictions.append(1 if final_probs[1] > 0.5 else 0)\n",
    "        confidences.append(max(final_probs))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, confidences)\n",
    "    pr_auc = average_precision_score(y_test, confidences)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    print(f\"TN: {cm[0,0]:6}  |  FP: {cm[0,1]:6}\")\n",
    "    print(f\"FN: {cm[1,0]:6}  |  TP: {cm[1,1]:6}\")\n",
    "    \n",
    "    print(\"\\n=== Performance Metrics ===\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return predictions, confidences\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_moe(X_test, y_test, expert_1, expert_2, gating_net, phrase_dict)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Save Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nSaving gating network...\")\n",
    "torch.save(gating_net.state_dict(), 'gating_network.pth')\n",
    "print(\"âœ… Gating network saved as 'gating_network.pth'\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: Prediction Function with Trained Gating Network\n",
    "# ============================================================================\n",
    "\n",
    "def predict_with_gating(text, url, gating_net, expert_1, expert_2, phrase_dict):\n",
    "    text = preprocess_text(text)\n",
    "    phrase_score = calculate_phrase_score(text, phrase_dict)\n",
    "    \n",
    "    # Get predictions\n",
    "    if url and url.strip():\n",
    "        try:\n",
    "            url_df = pd.DataFrame({'url': [url]})\n",
    "            url_probs = expert_1.predict_proba(url_df)[0]\n",
    "        except:\n",
    "            url_probs = np.array([0.5, 0.5])\n",
    "    else:\n",
    "        url_probs = np.array([0.5, 0.5])\n",
    "    \n",
    "    if text:\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, \n",
    "                             truncation=True, max_length=128)\n",
    "            with torch.no_grad():\n",
    "                outputs = expert_2(**inputs)\n",
    "                text_probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
    "        except:\n",
    "            text_probs = np.array([0.5, 0.5])\n",
    "    else:\n",
    "        text_probs = np.array([0.5, 0.5])\n",
    "    \n",
    "    gating_features = extract_gating_features(text, url, phrase_score)\n",
    "    gating_input = torch.FloatTensor(gating_features).unsqueeze(0)\n",
    "    \n",
    "    gating_net.eval()\n",
    "    with torch.no_grad():\n",
    "        expert_weights = gating_net(gating_input)\n",
    "    \n",
    "    final_probs = (expert_weights[0, 0].item() * url_probs + \n",
    "                  expert_weights[0, 1].item() * text_probs)\n",
    "    \n",
    "    prediction = \"PHISHING âš ï¸\" if final_probs[1] > 0.5 else \"SAFE âœ…\"\n",
    "    confidence = max(final_probs) * 100\n",
    "    \n",
    "    url_contrib = expert_weights[0, 0].item() * 100\n",
    "    text_contrib = expert_weights[0, 1].item() * 100\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence,\n",
    "        'url_weight': url_contrib,\n",
    "        'text_weight': text_contrib,\n",
    "        'url_prediction': 'PHISHING' if url_probs[1] > 0.5 else 'SAFE',\n",
    "        'text_prediction': 'PHISHING' if text_probs[1] > 0.5 else 'SAFE',\n",
    "        'phrase_score': phrase_score\n",
    "    }\n",
    "\n",
    "def test_sample(input_text):\n",
    "    \"\"\"Smart auto-detection for testing\"\"\"\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    urls = re.findall(url_pattern, input_text)\n",
    "    \n",
    "    if urls:\n",
    "        url = urls[0]\n",
    "        text = re.sub(url_pattern, '', input_text).strip()\n",
    "    else:\n",
    "        url = \"\"\n",
    "        text = input_text.strip()\n",
    "    \n",
    "    results = predict_with_gating(text, url, gating_net, expert_1, expert_2, phrase_dict)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ¯ PREDICTION RESULTS (with Trained Gating Network)\")\n",
    "    print(\"=\" * 70)\n",
    "    if text:\n",
    "        print(f\"ğŸ“ Text: {text[:80]}...\" if len(text) > 80 else f\"ğŸ“ Text: {text}\")\n",
    "    if url:\n",
    "        print(f\"ğŸ”— URL: {url}\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"ğŸ§  Gating Network Weights:\")\n",
    "    print(f\"  ğŸŒ URL Expert:  {results['url_weight']:.1f}%\")\n",
    "    print(f\"  ğŸ“„ Text Expert: {results['text_weight']:.1f}%\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"ğŸ¯ FINAL: {results['prediction']}\")\n",
    "    print(f\"ğŸ“Š Confidence: {results['confidence']:.2f}%\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"ğŸ‰ READY TO TEST! \".center(70, \"=\"))\n",
    "print(\"\"\"\n",
    "Use test_sample() to test with the TRAINED gating network:\n",
    "\n",
    "test_sample(\"URGENT! Click here http://paypa1.com\")\n",
    "test_sample(\"Hey, want to grab coffee?\")\n",
    "test_sample(\"http://suspicious-site.com\")\n",
    "\n",
    "The gating network now LEARNS which expert to trust! ğŸ§ \n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c53a8-8e03-4988-9ab0-507ccd590a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
